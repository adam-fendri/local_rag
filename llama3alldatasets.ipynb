{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:18:19.790270Z","iopub.execute_input":"2024-07-15T10:18:19.791017Z","iopub.status.idle":"2024-07-15T10:19:46.976857Z","shell.execute_reply.started":"2024-07-15T10:18:19.790982Z","shell.execute_reply":"2024-07-15T10:19:46.975687Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"%pip install -U bitsandbytes \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport torch\nimport wandb\nfrom datasets import load_dataset, concatenate_datasets\nfrom huggingface_hub import login\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    logging,\n    TrainerCallback \n)\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n)\nfrom trl import SFTTrainer, setup_chat_format\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-15T10:20:23.573238Z","iopub.execute_input":"2024-07-15T10:20:23.573603Z","iopub.status.idle":"2024-07-15T10:20:23.582186Z","shell.execute_reply.started":"2024-07-15T10:20:23.573576Z","shell.execute_reply":"2024-07-15T10:20:23.580499Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Set your Hugging Face token and WandB token\nhf_token = os.getenv(\"HUGGINGFACE_TOKEN\", \"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\")\nwandb_token = os.getenv(\"WANDB_TOKEN\", \"c3fb602caddeba8090c8842a9616b1a0fbd64d2a\")\n\n# Login to Hugging Face Hub\nlogin(token=hf_token)\n\n# Initialize WandB\nwandb.login(key=wandb_token)\nrun = wandb.init(\n    project='Fine-tune Llama 3 8B on Medical Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:20:29.901825Z","iopub.execute_input":"2024-07-15T10:20:29.902556Z","iopub.status.idle":"2024-07-15T10:20:53.162879Z","shell.execute_reply.started":"2024-07-15T10:20:29.902525Z","shell.execute_reply":"2024-07-15T10:20:53.161785Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:iq039i2w) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae72d15157404b74897b7fba1f1c4539"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fine-sky-10</strong> at: <a href='https://wandb.ai/adam-fendri/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/iq039i2w' target=\"_blank\">https://wandb.ai/adam-fendri/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/iq039i2w</a><br/> View project at: <a href='https://wandb.ai/adam-fendri/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset' target=\"_blank\">https://wandb.ai/adam-fendri/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240715_094620-iq039i2w/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:iq039i2w). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240715_102030-2vb1ztas</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/adam-fendri/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/2vb1ztas' target=\"_blank\">cool-lake-11</a></strong> to <a href='https://wandb.ai/adam-fendri/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/adam-fendri/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset' target=\"_blank\">https://wandb.ai/adam-fendri/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/adam-fendri/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/2vb1ztas' target=\"_blank\">https://wandb.ai/adam-fendri/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/2vb1ztas</a>"},"metadata":{}}]},{"cell_type":"code","source":"# Set base model and dataset\nbase_model = \"HPAI-BSC/Llama3-Aloe-8B-Alpha\"\nnew_model_base = \"llama-3-8b-chat-doctor\"\n\n# Set torch dtype and attention implementation based on GPU capability\nif torch.cuda.get_device_capability()[0] >= 8:\n    torch_dtype = torch.bfloat16\n    attn_implementation = \"flash_attention_2\"\nelse:\n    torch_dtype = torch.float16\n    attn_implementation = \"eager\"\n    \nprint(torch_dtype)    \n","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:40:25.787643Z","iopub.execute_input":"2024-07-13T16:40:25.788038Z","iopub.status.idle":"2024-07-13T16:40:25.796548Z","shell.execute_reply.started":"2024-07-13T16:40:25.788011Z","shell.execute_reply":"2024-07-13T16:40:25.795485Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"torch.float16\n","output_type":"stream"}]},{"cell_type":"code","source":"# 8-bit quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_8bit=True\n)\n\n# Load model with quantization\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:40:31.862482Z","iopub.execute_input":"2024-07-13T16:40:31.862880Z","iopub.status.idle":"2024-07-13T16:41:25.309049Z","shell.execute_reply.started":"2024-07-13T16:40:31.862851Z","shell.execute_reply":"2024-07-13T16:41:25.307902Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af338416bfa747a2acbc86115f0071b3"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\n\n# Setup chat format and apply LoRA config\nmodel, tokenizer = setup_chat_format(model, tokenizer)\nmodel = get_peft_model(model, peft_config)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:41:33.497831Z","iopub.execute_input":"2024-07-13T16:41:33.498537Z","iopub.status.idle":"2024-07-13T16:41:34.421780Z","shell.execute_reply.started":"2024-07-13T16:41:33.498504Z","shell.execute_reply":"2024-07-13T16:41:34.420610Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Datasets information\ndatasets_info = [\n    {\"number\": 1, \"name\": \"ruslanmv/ai-medical-chatbot\", \"instruction_key\": \"Patient\", \"output_key\": \"Doctor\"},\n    {\"number\": 2, \"name\": \"lavita/medical-qa-datasets\", \"config\": \"all-processed\", \"instruction_key\": \"input\", \"output_key\": \"output\"},\n    {\"number\": 3, \"name\": \"Malikeh1375/medical-question-answering-datasets\", \"config\": \"all-processed\", \"instruction_key\": \"input\", \"output_key\": \"output\"}\n]\n","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:41:36.850790Z","iopub.execute_input":"2024-07-13T16:41:36.851526Z","iopub.status.idle":"2024-07-13T16:41:36.858255Z","shell.execute_reply.started":"2024-07-13T16:41:36.851495Z","shell.execute_reply":"2024-07-13T16:41:36.857133Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Specify the dataset number here\ndataset_number = 1  # Change this number to train on a different dataset\n\n# Find the dataset info based on the dataset number\ndataset_info = next(item for item in datasets_info if item[\"number\"] == dataset_number)\n\n# Function to preprocess a single dataset\ndef preprocess_dataset(dataset_name, config, instruction_key, output_key):\n    dataset = load_dataset(dataset_name, config, split=\"all\")\n    dataset = dataset.map(lambda row: {\n        \"text\": tokenizer.apply_chat_template(\n            [{\"role\": \"user\", \"content\": row[instruction_key]},\n             {\"role\": \"assistant\", \"content\": row[output_key]}],\n            tokenize=False\n        )\n    }, num_proc=4)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:41:39.767301Z","iopub.execute_input":"2024-07-13T16:41:39.767976Z","iopub.status.idle":"2024-07-13T16:41:39.777273Z","shell.execute_reply.started":"2024-07-13T16:41:39.767941Z","shell.execute_reply":"2024-07-13T16:41:39.776102Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Function to create a smaller evaluation subset from the validation set\ndef create_eval_subset(dataset, size=100):\n    indices = random.sample(range(len(dataset)), size)\n    return dataset.select(indices)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:41:42.821644Z","iopub.execute_input":"2024-07-13T16:41:42.822040Z","iopub.status.idle":"2024-07-13T16:41:42.828891Z","shell.execute_reply.started":"2024-07-13T16:41:42.822009Z","shell.execute_reply":"2024-07-13T16:41:42.827835Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Load and preprocess the selected dataset\nconfig = dataset_info.get(\"config\", None)\ndataset = preprocess_dataset(dataset_info[\"name\"], config, dataset_info[\"instruction_key\"], dataset_info[\"output_key\"])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:41:44.658711Z","iopub.execute_input":"2024-07-13T16:41:44.659333Z","iopub.status.idle":"2024-07-13T16:41:48.094090Z","shell.execute_reply.started":"2024-07-13T16:41:44.659300Z","shell.execute_reply":"2024-07-13T16:41:48.092708Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Split the dataset into training and validation sets\ntrain_dataset = dataset.train_test_split(test_size=0.1)\nvalidation_dataset = train_dataset[\"test\"]","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:41:49.967298Z","iopub.execute_input":"2024-07-13T16:41:49.967694Z","iopub.status.idle":"2024-07-13T16:41:50.092063Z","shell.execute_reply.started":"2024-07-13T16:41:49.967663Z","shell.execute_reply":"2024-07-13T16:41:50.090987Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Define training arguments\ntraining_arguments = TrainingArguments(\n    output_dir=f\"{new_model_base}-{dataset_info['number']}\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=3,  # Start with 3 epochs\n    eval_strategy=\"steps\",  # Use eval_strategy instead of evaluation_strategy\n    eval_steps=400,  # Evaluate every 200 steps\n    save_steps=400,  # Save checkpoints every 200 steps\n    logging_steps=50,  # Log every 50 steps\n    warmup_steps=100,  # Adjust warmup steps\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    bf16=False,  # Use bf16 instead of fp16\n    group_by_length=True,\n    report_to=\"wandb\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:42:05.146116Z","iopub.execute_input":"2024-07-13T16:42:05.147017Z","iopub.status.idle":"2024-07-13T16:42:05.184702Z","shell.execute_reply.started":"2024-07-13T16:42:05.146983Z","shell.execute_reply":"2024-07-13T16:42:05.183727Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Custom callback to sample different evaluation subsets\nclass RandomSubsetEvalCallback(TrainerCallback):\n    def __init__(self, eval_dataset, subset_size):\n        self.eval_dataset = eval_dataset\n        self.subset_size = subset_size\n\n    def on_step_end(self, args, state, control, **kwargs):\n        if state.global_step % args.eval_steps == 0:\n            indices = random.sample(range(len(self.eval_dataset)), self.subset_size)\n            subset = self.eval_dataset.select(indices)\n            trainer.evaluate(eval_dataset=subset)\n        return control\n","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:42:08.101847Z","iopub.execute_input":"2024-07-13T16:42:08.102207Z","iopub.status.idle":"2024-07-13T16:42:08.110773Z","shell.execute_reply.started":"2024-07-13T16:42:08.102177Z","shell.execute_reply":"2024-07-13T16:42:08.109623Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Setting up SFT trainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset[\"train\"],\n    eval_dataset=validation_dataset,  # Use the full validation dataset for sampling\n    peft_config=peft_config,\n    max_seq_length=512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=False,\n    callbacks=[RandomSubsetEvalCallback(validation_dataset, 100)]  # Custom callback for random subsets\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:42:10.824823Z","iopub.execute_input":"2024-07-13T16:42:10.825631Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/231224 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54c792e2f9ba4c618327d1446dd8b481"}},"metadata":{}}]},{"cell_type":"code","source":"# Start training\ntrainer.train()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model to Hugging Face Hub\ntrainer.save_model(new_model)\ntokenizer.save_pretrained(new_model)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Push the model and tokenizer to Hugging Face Hub\nmodel.push_to_hub(new_model, use_auth_token=hf_token)\ntokenizer.push_to_hub(new_model, use_auth_token=hf_token)\n","metadata":{},"execution_count":null,"outputs":[]}]}